\lesson{2}{28 Feb 2023}{}

\section{Fitting and Assessing GLMs}
In week 2 of the course, we cover the procedure for fitting GLMs, followed by assessing their goodness of fit. The lectures cover some interesting asymptotic properties of the distribution of fitted parameters, and fit-assessment statistics. Covering asymptotic distributions rigourously however is considered out of scope.

\subsection{Fitting GLMs}
Having specified a GLM with design vector $z$, we will express our data set as $(y_i, z_i(x_i)))$ for $i=1,\dots,n$. In matrix-vector form, we can write these as $y$ and $Z$ where $Z = (z_1, \dots, z_n)^T\in\mathbb{R}^{n\times p}$. \\
\\
The regression coefficients of the model $\beta$ are typically found via maximum likelihood estimation (MLE). Deriving MLE for GLMs is not as straightforward, as a root-solving routine is typically required to find the roots of $\ell^\prime(\beta)$.

\begin{definition}[Score Equation]
    
\end{definition}

\begin{theorem}[Properties of MLE estimates]
    Under regularity assumptions $\hat{\beta}$ has the following \textit{nice} properties:\begin{enumerate}
        \item \textit{Asympotic Existance and Uniqueness}. With probability 1 as $n\rightarrow\infty$ $\hat{\beta}$ exists and is locally unique
        \item \textit{Consistency}. $\hat{\beta}\rightarrow\beta$ with probability 1. 
        \item \textit{Asymptotically Normal}. $\hat{\beta}\sim N(\beta, F^{-1}(\beta))$. In practice, one typically assumes that $cov(\hat{\beta}) \approx F^{-1}(\beta)$ so $\hat{\beta}\sim N(\beta, cov(\hat{\beta}))$
    \end{enumerate}
\end{theorem}
\begin{proof}
    Requires a course in asymptotic statistics to prove rigorously, but we can given an \textit{intuitive} argument below.
\end{proof}

\begin{lemma}[MLE Estimate of $\phi$]
    A consistent estimator for the dispersion parameter $\phi$ is 
    $$\hat{\phi} = \frac{1}{n-p}\sum_{i=1}^{n}\frac{y_i - \hat{\mu_i}}{v(\mu_i)}$$
\end{lemma}
\begin{note}
    We only need an estimate for $\phi$ if we require an estimate for the Fisher Information Matrix. This typically arises when constructing confidence intervals.
\end{note}

\subsection{Hypothesis Testing}
Having fit a GLM for regression coefficients $\beta$, a common task is to hypothesis test significance of the coefficients. A typical test is of the form: 
$$H_0: \beta_r = 0 \hspace{10mm} vs. \hspace{10mm} H_1: \beta_r \neq 0$$
where $\beta_r$ is a subvector of $\beta$. That is, a test of the signficance of some subset of some covariates of the model.

\subsection{Assessing Model Fit}
For a standard linear model, one typically considers the sum of squared residuals (SSE) as a measure of model fit
\begin{align*}
    SSE &= \sum_{i=1}^{n}(y_i - \hat{y_i})^2 \\
    &= (y - Z\beta)^T(y-Z\beta)
\end{align*}
Notice that this places eqaul weight on each residual $y_i - 
\hat{y_i}$, which is not quite as reasonable if the variance of each sample varies with $\hat{y_i}$ as is the case for GLMs. The two alternatives for GLMs is the \textit{Devience} and \textit{Pearson Statistic}.

\begin{definition}[Pearson's $\chi^2$ Statistic]
    For grouped data, Pearsons statistic is 
    $$\chi^2 = \sum_{i=1}^{g}\frac{(\Bar{y_i} - \hat{y_i})^2}{v(\hat{\mu_i})/n_i}$$
    where $n_i$ is the size of group $i$.
\end{definition}

\subsection{Appendix of Algebraic Results}
